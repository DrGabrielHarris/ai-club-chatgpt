{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(\".env\")\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=100,\n",
    "        temperature=0.9,\n",
    "        response_format={\n",
    "            \"type\": \"text\",\n",
    "        },\n",
    "    )\n",
    "\n",
    "    completion = completion.to_dict()\n",
    "\n",
    "    content = completion[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "    return content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. **Provide Examples**\n",
    "\n",
    "* Our prompts didn't give any examples of what \"good\" outputs look like! A prompt with no examples is called a `zero-shot` prompt which is asking for a lot without giving much in return.\n",
    "\n",
    "* Even adding one example, which is called `one-shot`, helps considerably\n",
    "\n",
    "* It's more common practice to add multiple examples, which is called `few-shot`. The strength of a prompt often comes down to the examples used, especially if you're not a domain expert as providing examples can sometimes be easier than trying to explain what is it that you like about the output to look like "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">\n",
    "Task: experiment with the number of examples, from <strong>zero-shot</strong> to <strong>few-shot</strong>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Brainstorm a list of product names for a pair of trainers that appeal to teenagers, in the style of Nike.\n",
    "\n",
    "Return the response as a comma-separated list of 5 names, in this format: [\"Name 1\", \"Name 2\", \"Name 3\", \"Name 4\", \"Name 5\"]\n",
    "\n",
    "Examples: \n",
    "[\"Air Max\", \"Air Force\", \"Blazer\", \"Dunk\", \"Cortez\"]\n",
    "\"\"\"\n",
    "\n",
    "completion = get_completion(prompt)\n",
    "completion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Trade-off**: between reliability and creativity. Go past 3-5 examples and the results will become more reliable but less creative. Give similar examples and the results will be much more constrained and less diverse. Lack of diversity and variations can cause a problem in handling edge cases."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
