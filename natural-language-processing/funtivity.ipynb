{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from spacy import displacy\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Task: load the `spacy` model `en_core_web_sm-3.7.0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Task code here:\"\"\"\n",
    "nlp = spacy.load(\n",
    "    Path(\n",
    "        \"natural-language-processing\",\n",
    "        \"models\",\n",
    "        \"en_core_web_sm-3.7.0\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at the `Harry Potter and the Philosopher's Stone` movie transcript in the `data` folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Task: read the text file into a list of lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Task code here:\"\"\"\n",
    "with Path(\n",
    "    \"natural-language-processing\",\n",
    "    \"data\",\n",
    "    \"Harry Potter and the Philosopher's Stone.txt\",\n",
    ").open(\"r\", encoding=\"utf-8\") as file:\n",
    "    lines = file.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Task: keep only characters lines. These start with `Character's name:` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Task code here:\"\"\"\n",
    "script = []\n",
    "\n",
    "for line in lines:\n",
    "    if \":\" in line:\n",
    "        script.append(line.rstrip(\"\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Task: amend your code to remove scene description inside a line. These are encapsulated in `[]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Task code here:\"\"\"\n",
    "script = []\n",
    "\n",
    "for line in lines:\n",
    "    if \":\" in line:\n",
    "        if \"[\" in line:\n",
    "            line = re.sub(r\"\\[.*\\]\", \"\", line)\n",
    "            script.append(line.rstrip(\"\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Task: define a function that uses spacy to tokenize a document, excluding punctuation and stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Task code here:\"\"\"\n",
    "\n",
    "\n",
    "def tokenize(document):\n",
    "    doc = nlp(document)\n",
    "\n",
    "    tokens = []\n",
    "\n",
    "    for token in doc:\n",
    "        if not token.is_punct and not token.is_stop:\n",
    "            tokens.append(token.text)\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Task: tokenize each document and create a corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Task code here:\"\"\"\n",
    "corpus = []\n",
    "\n",
    "for line in script:\n",
    "    tokens = tokenize(line)\n",
    "    if tokens:\n",
    "        corpus.append(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Task 1: get a list of all the characters (these are the first item in each document)\n",
    "* Task 2: get a set of *unique* characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Task 1 code here:\"\"\"\n",
    "characters = []\n",
    "\n",
    "for line in corpus:\n",
    "    characters.append(line[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Task 2 code here:\"\"\"\n",
    "characters_unique = set(characters)\n",
    "characters_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Task 1: build a Gensim dictionary from corpus\n",
    "* Task 2: print out each token and it's id found in the dictionary\n",
    "* Task 3: print out the top 10 most common tokens in the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Task 1 code here:\"\"\"\n",
    "dictionary = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Task 2 code here:\"\"\"\n",
    "for token_id, token in ...:\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Task 3 code here:\"\"\"\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Task 1: print out each token frequency across all documents (i.e. collection frequencies)\n",
    "* Task 2: print out the number of documents in the corpus \n",
    "* Task 3: print out the number of terms in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Task 1 code here:\"\"\"\n",
    "cfs = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Task 2 code here:\"\"\"\n",
    "num_docs = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Task 3 code here:\"\"\"\n",
    "num_terms = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Task 1: generate a [wordcloud](https://github.com/amueller/word_cloud) of the top 50 most frequent tokens in the script\n",
    "* Task 2: use [matplotlib](https://matplotlib.org/) to plot the generated wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Task 1 code here:\"\"\"\n",
    "word_cloud = WordCloud(\n",
    "    max_words=50,\n",
    "    background_color=\"white\",\n",
    "    colormap=\"magma\",\n",
    "    contour_width=1,\n",
    "    contour_color=\"orange\",\n",
    "    relative_scaling=0.5,\n",
    ")\n",
    "\n",
    "wc = word_cloud.generate(\" \".join(characters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Task 2 code here:\"\"\"\n",
    "_, ax = plt.subplots(figsize=(15, 8))\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Task: use spacy to predict all the entities in the first 10 lines of the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in script[:9]:\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Task: use [displacy](https://spacy.io/usage/visualizers/) to visualize the entities in line number 10 in the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(script[9])\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
