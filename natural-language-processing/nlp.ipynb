{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import spacy\n",
    "# from gensim.corpora.dictionary import Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Spacy](https://spacy.io/usage/spacy-101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"The different unites into which you can break a text into are called tokens. A token could be a character, a punctuation, an emoji, a word (also called gram).\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Task 1: load the `spacy` model `en_core_web_sm-3.7.0`\n",
    "* Task 2: create a spacy `doc` from `text`\n",
    "* Task 3: print out each token `text`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Task 1 code here:\"\"\"\n",
    "nlp = spacy.load(\n",
    "    Path(\n",
    "        \"natural-language-processing\",\n",
    "        \"models\",\n",
    "        \"en_core_web_sm-3.7.0\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Task 2 code here:\"\"\"\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Task 3 code here:\"\"\"\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Task 1: print out punctuation tokens\n",
    "* Task 2: remove punctuation tokens from text and save them into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Task 1 code here:\"\"\"\n",
    "for token in doc:\n",
    "    if token.is_punct:\n",
    "        print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Task 2 code here:\"\"\"\n",
    "tokens = []\n",
    "punctuations = []\n",
    "\n",
    "for token in doc:\n",
    "    if token.is_punct:\n",
    "        print(token.text)\n",
    "        punctuations.append(token.text)\n",
    "    else:\n",
    "        tokens.append(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Task 1: print out stop words\n",
    "* Task 2: remove stop words tokens from text and save them into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Task 1 code here:\"\"\"\n",
    "for token in doc:\n",
    "    if token.is_stop:\n",
    "        print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Task 2 code here:\"\"\"\n",
    "tokens = []\n",
    "stop_words = []\n",
    "\n",
    "for token in doc:\n",
    "    if token.is_stop:\n",
    "        stop_words.append(token.text)\n",
    "    else:\n",
    "        tokens.append(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Gensim](https://radimrehurek.com/gensim/intro.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The core concepts of gensim are:\n",
    "\n",
    "* Document: some text.\n",
    "* Corpus: a collection of documents.\n",
    "* Vector: a mathematically convenient representation of a document.\n",
    "* Model: an algorithm for transforming vectors from one representation to another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_1 = \"Python: Hands on Machine Learning with Python\"\n",
    "document_2 = \"Deep Learning with Python\"\n",
    "document_3 = \"Python for Data Analysis\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Task 1: define a function that uses spacy to tokenize a document, excluding punctuation and stop words\n",
    "* Task 2: use the function to tokenize the documents\n",
    "* Task 3: create a corpus from the tokenizes documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Task 1 code here:\"\"\"\n",
    "\n",
    "\n",
    "def tokenize(document):\n",
    "    ...\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Task 2 code here:\"\"\"\n",
    "document_1_tokens = tokenize(document_1)\n",
    "document_2_tokens = tokenize(document_2)\n",
    "document_3_tokens = tokenize(document_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Task 3 code here:\"\"\"\n",
    "corpus = [...]\n",
    "\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dictionary: a dictionary defines the vocabulary of all tokens, and a mapping between tokens and their integer ids\n",
    "\n",
    "* Task 1: build a Gensim dictionary from corpus\n",
    "* Task 2: print out each token and it's id found in the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Task 1 code here:\"\"\"\n",
    "dictionary = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Task 2 code here:\"\"\"\n",
    "for token_id, token in dictionary.items():\n",
    "    print(token_id, token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Task 1: convert the first document into a list of ids\n",
    "* Task 2: get the token for each of the ids "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Task 1 code here:\"\"\"\n",
    "ids = ...\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Task 2 code here:\"\"\"\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of words (bow): each document is represented by a vector containing the frequency counts of each word in the dictionary. The order of the tokens in the document that is encoded is ignored, which is where the name bag-of-words comes from\n",
    "\n",
    "* Task 1: create a bow representation of the corpus\n",
    "* Task 2: the first document bow representation, get the token for each of the ids \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Task 1 code here:\"\"\"\n",
    "corpus_bow = [...]\n",
    "corpus_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Task 2 code here:\"\"\"\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
