{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import tiktoken\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(\".env\")\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"write a dad joke\"\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt,\n",
    "        }\n",
    "    ],\n",
    "    max_tokens=100,\n",
    "    temperature=0.9,\n",
    ")\n",
    "\n",
    "completion = completion.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a prompt?\n",
    "\n",
    "The input you provide, typically text, when interfacing with an AI model. It serves as a set of instructions the model uses to predict the desired response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is prompt engineering?\n",
    "\n",
    "The process of discovering prompts that reliably yield useful or desired results.\n",
    "\n",
    "_Average prompts will return average responses!_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a model?\n",
    "\n",
    "The OpenAI API is powered by a diverse set of [models](https://platform.openai.com/docs/models) with different capabilities and [pricing](https://openai.com/api/pricing/). \n",
    "\n",
    "Each model is being updated frequently. If you don't specify the version of the model, you will be using the `latest` version. \n",
    "\n",
    "To specify the model version, we can suffix it to the base model name, e.g., `gpt-4o-mini-2024-07-18`\n",
    "\n",
    "**Task**: Find the version of the model we used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion[\"...\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a token?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokens can be thought of as pieces of words. These tokens are not cut up exactly where the words start or end - tokens can include trailing spaces and even sub-words. \n",
    "\n",
    "Here are some helpful rules of thumb for understanding tokens in terms of lengths:\n",
    "\n",
    "* 1 token ~= 4 chars in English\n",
    "* 1 token ~= Â¾ words\n",
    "* 100 tokens ~= 75 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Find the number of tokens in the above example for:\n",
    "\n",
    "1. The prompt that you sent\n",
    "2. The completion that you received\n",
    "3. The total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion[\"usage\"][\"...\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion[\"usage\"][\"...\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion[\"usage\"][\"...\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: can you confirm these numbers using the OpenAI \n",
    "[tokenizer](https://platform.openai.com/tokenizer) UI?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: can you use [tiktoken](https://github.com/openai/tiktoken) library to count the number of prompt tokens programmatically?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tiktoken.encoding_for_model(\"...\")\n",
    "tokens = encoding.encode(...)\n",
    "len(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Token Limits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the [model](https://platform.openai.com/docs/models) used, requests can use up to 128,000 tokens shared between prompt and completion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Token Pricing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requests to different models are priced differently. You can find details on token pricing [here](https://openai.com/api/pricing/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is temperature?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can think of temperature like randomness, with 0 being least random (most deterministic) and 2 being most random (least deterministic). When using low values for temperature (e.g. 0.2) the model responses will tend to be more consistent but may feel more robotic. Values higher than 1.0, especially values close to 2.0, can lead to erratic model outputs. If your goal is creative outputs, a combination of a slightly higher than normal temperature (e.g. 1.2) combined with a prompt specifically asking the model to be creative may be your best bet, but we encourage experimentation.\n",
    "\n",
    "**Task**: Change the temperature above and resubmit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"write a dad joke and be creative\"\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt,\n",
    "        }\n",
    "    ],\n",
    "    max_tokens=100,\n",
    "    temperature=1.5,\n",
    ")\n",
    "\n",
    "completion = completion.to_dict()\n",
    "\n",
    "completion[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Create a `get_completion` function to send a prompt to a specific model and to return the response content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=\"gpt-4o-mini\"):\n",
    "    completion = ...\n",
    "\n",
    "    completion = completion.to_dict()\n",
    "\n",
    "    content = completion[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "    return content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
