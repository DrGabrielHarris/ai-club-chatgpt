{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import tiktoken\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(\".env\")\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"write a dad joke\"\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt,\n",
    "        }\n",
    "    ],\n",
    "    max_tokens=100,\n",
    "    temperature=0.9,\n",
    ")\n",
    "\n",
    "completion = completion.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a prompt?\n",
    "\n",
    "* The **input** you provide, which is typically a **text**, but it could also be an **image** or an **audio** file. \n",
    "* It serves as a set of **instructions** the **model** uses to predict the desired response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is prompt engineering?\n",
    "\n",
    "* The process of **discovering prompts** that reliably returns useful or desired results.\n",
    "* Also known as **instructions-tuning**\n",
    "\n",
    "<div style=\"text-align: center; color: red;\">Average prompts >  average responses!</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a model?\n",
    "\n",
    "* The OpenAI API offers a diverse set of [models](https://platform.openai.com/docs/models) with different capabilities and [pricing](https://openai.com/api/pricing/).\n",
    "\n",
    "* Each model is updated frequently. If you don't specify the version of the model, you will be using the `latest` version. \n",
    "\n",
    "* To specify the model version, we can suffix it to the base model name. For example, to use the `2024-07-18` version of `gpt-4o-mini`, we should use: `gpt-4o-mini-2024-07-18`. \n",
    "\n",
    "* To find all the available versions of a model, search for that model. For example: [gpt-4o-mini](https://platform.openai.com/docs/models#gpt-4o-mini)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">\n",
    "Task: Find the version of the model we used\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion[\"model\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a token?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokens can be thought of as pieces of words. These tokens are not cut up exactly where the words start or end - tokens can include trailing spaces and even sub-words. \n",
    "\n",
    "Here are some helpful rules of thumb for understanding tokens in terms of lengths:\n",
    "\n",
    "* 1 token ~= 4 chars in English\n",
    "* 1 token ~= Â¾ words\n",
    "* 100 tokens ~= 75 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">\n",
    "Task: Find the number of tokens in the above example for:\n",
    "\n",
    "1. The prompt that you sent\n",
    "2. The completion that you received\n",
    "3. The total\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion[\"usage\"][\"prompt_tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion[\"usage\"][\"completion_tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion[\"usage\"][\"total_tokens\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">\n",
    "Task: can you confirm these numbers using the OpenAI \n",
    "<a href=\"https://platform.openai.com/tokenizer\" style=\"color:blue\">tokenizer</a> UI?\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">\n",
    "Task: can you use\n",
    "<a href=\"https://github.com/openai/tiktoken\" style=\"color:blue\">tiktoken</a> library to count the number of prompt tokens programmatically?\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
    "tokens = encoding.encode(prompt)\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Token Limits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the [model](https://platform.openai.com/docs/models) used, requests can use up to 128,000 tokens shared between prompt and completion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Token Pricing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requests to different models are priced differently. You can find details on token pricing [here](https://openai.com/api/pricing/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is temperature?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* You can think of temperature like randomness, with 0 being least random (most deterministic) and 2 being most random (least deterministic). \n",
    "\n",
    "* When using low values for temperature (e.g. 0.2) the model responses will tend to be more consistent but may feel more robotic. \n",
    "\n",
    "* Values higher than 1.0, especially values close to 2.0, can lead to erratic model outputs. \n",
    "\n",
    "* If your goal is creative outputs, a combination of a slightly higher than normal temperature (e.g. 1.2) combined with a prompt specifically asking the model to be creative may be your best bet, but you should experiment!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">\n",
    "Task: Change the temperature above and resubmit\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"write a dad joke and be creative\"\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt,\n",
    "        }\n",
    "    ],\n",
    "    max_tokens=100,\n",
    "    temperature=1.5,\n",
    ")\n",
    "\n",
    "completion = completion.to_dict()\n",
    "\n",
    "completion[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">\n",
    "Task: Create a <span style=\"color:blue\">get_completion</span> function to send a prompt to a specific model and to return the response content\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=\"gpt-4o-mini\"):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=100,\n",
    "        temperature=0.9,\n",
    "    )\n",
    "\n",
    "    completion = completion.to_dict()\n",
    "\n",
    "    content = completion[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "    return content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
