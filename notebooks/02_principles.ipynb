{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import json\n",
    "import os\n",
    "\n",
    "import ipywidgets as widgets\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(\".env\")\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=100,\n",
    "        temperature=0.9,\n",
    "        response_format={\n",
    "            \"type\": \"text\",\n",
    "        },\n",
    "    )\n",
    "\n",
    "    completion = completion.to_dict()\n",
    "\n",
    "    content = completion[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "    return content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average prompt, average response\n",
    "\n",
    "**Task**: run the following prompt and examine the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Brainstorm a list of product names for a pair of trainers that appeal to teenagers?\n",
    "\"\"\"\n",
    "\n",
    "get_completion(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: what do you think some of the obvious issues with this prompt?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Vague direction**: what style and attributes?\n",
    "2. **Unformatted output**: how many names and in what format?\n",
    "3. **Missing example**: what good names look like?\n",
    "4. **Limited evaluation**: how to determine if a name is good or bad?\n",
    "5. **No task division**: what are the steps involved?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Five Principles of Prompting\n",
    "\n",
    "1. **Give Direction**: describe the desired style in detail, or reference a relevant persona\n",
    "\n",
    "2. **Specify Format**: define what rules to follow, and the required structure of the response\n",
    "\n",
    "3. **Provide Examples**: insert a diverse set of test cases where the task was done correctly\n",
    "\n",
    "4. **Evaluate Quality**: identify errors and rate responses, testing what drives performance\n",
    "\n",
    "5. **Divide Labour**: split tasks into multiple steps, chained together for complex goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. **Give Direction**\n",
    "\n",
    "Human would also struggle to complete this task without a good brief. Imagine what context a human might need for this task and try including it in the prompt.\n",
    "\n",
    "**Strategies**:\n",
    "\n",
    "* Role-playing\n",
    "* Prewarming or internal retrieval\n",
    "* Best advice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Role-playing**: find a persona (who is famous in the training data) to emulate their style\n",
    "\n",
    "**Task**: Use your favourite persona/brand to give direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Brainstorm a list of product names for a pair of trainers that appeal to teenagers?, in the style of Nike?\"\n",
    "get_completion(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prewarming or internal retrieval**: ask chatGPT for best practice advice, then ask it to follow its own advice \n",
    "\n",
    "**Task**: ask chatGPT for advice, then use its advice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Please give me 5 tips for naming products based on expert industry advice\"\n",
    "advice = get_completion(prompt)\n",
    "advice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Using the following advice:\n",
    "\n",
    "{advice}\n",
    "\n",
    "Brainstorm a list of product names for a pair of trainers that appeal to teenagers?, in the style of Nike?\n",
    "\"\"\"\n",
    "\n",
    "get_completion(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Best advice**: take the best advice you can find for the task and use it\n",
    "\n",
    "**Task**: Use the [5 Golden Rules](https://www.brandwatch.com/blog/how-to-name-a-product-our-5-golden-rules/) for naming a product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "advice = \"\"\"\n",
    "1. It should be readable and writable\n",
    "If your product name is hard to pronounce, people won’t talk about it and if they can’t write it down (and spell it correctly!) when they hear it, how do you expect them to Google it?\n",
    "\n",
    "Keep it simple and don’t go with any wacky spellings just for the sake of it.\n",
    "\n",
    "2. It should be unique\n",
    "It’s very hard in this day and age to be completely unique, so you can give yourself a bit of leeway, but your product name should at least be unique to your industry.\n",
    "\n",
    "This makes it much easier to get the domain, do well in search and know that when someone says the name, they mean your product.\n",
    "\n",
    "3. It should be short, punchy and memorable\n",
    "The longer the name, the harder it is to grab people.\n",
    "\n",
    "Longer names also mean people resort to abbreviations that you often don’t get to control.\n",
    "\n",
    "4. It should look good written down and sound cool to say\n",
    "You want your product name to jump off the page and stand out next to all the other boring words around it.\n",
    "\n",
    "When someone says it in a sentence it should stand out so everyone around pays attention.\n",
    "\n",
    "5. It should evoke an emotion, feeling or idea\n",
    "Your product name should tie back into what your product is, what the feeling you want people to have when experiencing your product is, and/or what idea are you trying to get across.\n",
    "\n",
    "It should be emotive and inspiring\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Using the following 5 golden rules:\n",
    "\n",
    "{advice}\n",
    "\n",
    "Brainstorm a list of product names for a pair of trainers that appeal to teenagers?, in the style of Nike?\n",
    "\"\"\"\n",
    "\n",
    "get_completion(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Trade-off**: too much direction can cause the model to quickly get into a conflicting combination that it can't resolve. While too much direction can narrow the creativity of the model, too little direction is the more common problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. **Specify Format**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AI models are universal translators! They can translate between:\n",
    "\n",
    "* Languages, for example French to English\n",
    "* Data formats, for example lists to json \n",
    "* Programming languages, for example, C# to Python\n",
    "* Natural language and programming languages, for example text to Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Task**: Update the prompt to return the response as a comma-separated list of 5 names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Brainstorm a list of product names for a pair of trainers that appeal to teenagers?, in the style of Nike?\n",
    "\n",
    "Return the response as a comma-separated list of 5 names, in this format: [\"Name 1\", \"Name 2\", \"Name 3\", \"Name 4\", \"Name 5\"]\n",
    "\"\"\"\n",
    "\n",
    "completion = get_completion(prompt)\n",
    "completion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Task**: Print the items in the list, one by one\n",
    "* *Hint*: the response is a string list, use `ast.literal_eval` first to convert it into a list object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = ast.literal_eval(completion)\n",
    "\n",
    "for item in completion:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. **Provide Examples**\n",
    "\n",
    "* Our prompts didn't give any examples of what \"good\" outputs look like! A prompt with no examples is called a `zero-shot` prompt which is asking for a lot without giving much in return.\n",
    "\n",
    "* Even adding one example, which is called `one-shot`, helps considerably\n",
    "\n",
    "* It's more common practice to add multiple examples, which is called `few-shot`. The strength of a prompt often comes down to the examples used, especially if you're not a domain expert as providing examples can sometimes be easier than trying to explain what is it that you like about the output to look like "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Task**: experiment with the number of examples, from `zero-shot` to `few-shot`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "completion = get_completion(prompt)\n",
    "completion = json.loads(completion)\n",
    "\n",
    "completion[\"ProductNames\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Trade-off**: between reliability and creativity. Go past 3-5 examples and the results will become more reliable but less creative. Give similar examples and the results will be much more constrained and less diverse. Lack of diversity and variations can cause a problem in handling edge cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. **Evaluate Quality**\n",
    "\n",
    "* So far, there has been no feedback loop to judge the quality of the responses other than checking the results manually (A.K.A vibe). \n",
    "\n",
    "* This is called **blind prompting** and it's only fine when our prompts are used temporally for a single task and rarely visited.\n",
    "\n",
    "* If we are planning to use the same prompt for an application, then we need to be more rigorous with measuring the results. \n",
    "\n",
    "* This is because when a new LLM model is released, or a new version of the same LLM model is released, there is no guarantee a prompt that works on the older model/version would work on the new model/version.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to evaluate?\n",
    "\n",
    "* **Step 1**: Generate a number of prompts for a task\n",
    "* **Step 2**: get a response for *multiple* runs for each prompt \n",
    "* **Step 3**: save results in a file\n",
    "* **Step 4**: evaluate each of the responses using one of the below strategies, which is best done blind and randomised to avoid favouring one prompt over another\n",
    "\n",
    "**Strategies**:\n",
    "\n",
    "* Simple thumps-up/down rating system\n",
    "* 3, 5, or 10 points rating system \n",
    "* Use a ground-truth\n",
    "* Use an LLM as judge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: \n",
    "* Iterate through **2 prompts**\n",
    "* For each prompt, iterate through **3 runs** \n",
    "* Give each prompt and run a name\n",
    "* Get a response from the model\n",
    "* Convert responses into a dataframe\n",
    "* Save the dataframe as a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_1 = \"\"\"Product description: A pair of shoes that can fit any foot size.\n",
    "Seed words: adaptable, fit, omni-fit.\n",
    "Product names:\"\"\"\n",
    "\n",
    "prompt_2 = \"\"\"Product description: A home milkshake maker.\n",
    "Seed words: fast, healthy, compact.\n",
    "Product names: HomeShaker, Fit Shaker, QuickShake, Shake Maker\n",
    "\n",
    "Product description: A watch that can tell accurate time in space.\n",
    "Seed words: astronaut, space-hardened, eliptical orbit\n",
    "Product names: AstroTime, SpaceGuard, Orbit-Accurate, EliptoTime.\n",
    "\n",
    "Product description: A pair of shoes that can fit any foot size.\n",
    "Seed words: adaptable, fit, omni-fit.\n",
    "Product names:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [prompt_1, prompt_2]\n",
    "responses = []\n",
    "no_runs = 3\n",
    "\n",
    "for ind, prompt in enumerate(prompts):\n",
    "    for run in range(no_runs):\n",
    "        variant = f\"{ind}_{run}\"\n",
    "        response = get_completion(prompt)\n",
    "        variant = f\"{ind}_{run}\"\n",
    "\n",
    "        data = {\n",
    "            \"variant\": variant,\n",
    "            \"prompt\": prompt,\n",
    "            \"response\": response,\n",
    "        }\n",
    "\n",
    "        responses.append(data)\n",
    "\n",
    "df = pd.DataFrame(responses)\n",
    "\n",
    "df.to_csv(\"responses.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the responses.csv file\n",
    "df = pd.read_csv(\"responses.csv\")\n",
    "\n",
    "# Shuffle the dataframe\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# df is your dataframe and 'response' is the column with text to test\n",
    "response_index = 0\n",
    "# add a new column to store feedback\n",
    "df[\"feedback\"] = pd.Series(dtype=\"str\")\n",
    "\n",
    "\n",
    "def on_button_clicked(b):\n",
    "    global response_index\n",
    "    #  convert thumbs up / down to 1 / 0\n",
    "    user_feedback = 1 if b.description == \"\\U0001f44d\" else 0\n",
    "\n",
    "    # update the feedback column\n",
    "    df.at[response_index, \"feedback\"] = user_feedback\n",
    "\n",
    "    response_index += 1\n",
    "    if response_index < len(df):\n",
    "        update_response()\n",
    "    else:\n",
    "        df.to_csv(\"results.csv\", index=False)\n",
    "\n",
    "        print(\"A/B testing completed. Here's the results:\")\n",
    "        # Calculate score and num rows for each variant\n",
    "        summary_df = (\n",
    "            df.groupby(\"variant\")\n",
    "            .agg(count=(\"feedback\", \"count\"), score=(\"feedback\", \"mean\"))\n",
    "            .reset_index()\n",
    "        )\n",
    "        print(summary_df)\n",
    "\n",
    "\n",
    "def update_response():\n",
    "    new_response = df.iloc[response_index][\"response\"]\n",
    "    if pd.notna(new_response):\n",
    "        new_response = \"<p>\" + new_response + \"</p>\"\n",
    "    else:\n",
    "        new_response = \"<p>No response</p>\"\n",
    "    response.value = new_response\n",
    "    count_label.value = f\"Response: {response_index + 1}\"\n",
    "    count_label.value += f\"/{len(df)}\"\n",
    "\n",
    "\n",
    "response = widgets.HTML()\n",
    "count_label = widgets.Label()\n",
    "\n",
    "update_response()\n",
    "\n",
    "thumbs_up_button = widgets.Button(description=\"\\U0001f44d\")\n",
    "thumbs_up_button.on_click(on_button_clicked)\n",
    "\n",
    "thumbs_down_button = widgets.Button(description=\"\\U0001f44e\")\n",
    "thumbs_down_button.on_click(on_button_clicked)\n",
    "\n",
    "button_box = widgets.HBox([thumbs_down_button, thumbs_up_button])\n",
    "\n",
    "display(response, button_box, count_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Divide Labour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Whe your prompt gets longer and more convoluted, the response might get less deterministic, and hallucinations or anomalies increase.\n",
    "\n",
    "* One of the core principles of engineering is to use *task decomposition* to break a problem down into their component parts, so we can easily solve each individual part.\n",
    "\n",
    "* Breaking your AI work into multiple calls that are chained together can help accomplishing more complex tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**:\n",
    "* Step 1: Generate a list of 10 product names (as before) \n",
    "\n",
    "* Step 2: Rate this list out of 10\n",
    "\n",
    "* Step 3: Append `let's think step by step` to understand how the rates where generated\n",
    "\n",
    "* Step 4: Choose the best product name with the highest rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "names = get_completion(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Rate this list of product names for a pair of shoes that can fit any foot size. The rating should be out of 10, inline next to the product name:\n",
    "\n",
    "{names} \n",
    "\"\"\"\n",
    "\n",
    "rating = get_completion(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Let's think step by step. Rate this list of product names for a pair of shoes that can fit any foot size. The rating should be out of 10, inline next to the product name:\n",
    "\n",
    "{names} \n",
    "\"\"\"\n",
    "\n",
    "rating = get_completion(prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
