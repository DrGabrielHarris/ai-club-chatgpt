{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import tiktoken\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(\".env\")\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"write a dad joke\"\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt,\n",
    "        }\n",
    "    ],\n",
    "    max_tokens=100,\n",
    "    temperature=0.9,\n",
    ")\n",
    "\n",
    "completion = completion.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a prompt?\n",
    "\n",
    "* The **input** you provide, which is typically a **text**, but it could also be an **image** or an **audio** file. \n",
    "* It serves as a set of **instructions** the **model** uses to predict the desired response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is prompt engineering?\n",
    "\n",
    "* The process of **discovering prompts** that reliably returns useful or desired results.\n",
    "* Also known as **instructions-tuning**\n",
    "\n",
    "<div style=\"text-align: center; color: red;\">Average prompts >  average responses!</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a model?\n",
    "\n",
    "* The OpenAI API offers a diverse set of [models](https://platform.openai.com/docs/models) with different capabilities and [pricing](https://openai.com/api/pricing/).\n",
    "\n",
    "* Each model is updated frequently. If you don't specify the version of the model, you will be using the `latest` version. \n",
    "\n",
    "* To specify the model version, we can suffix it to the base model name. For example, to use the `2024-07-18` version of `gpt-4o-mini`, we should use: `gpt-4o-mini-2024-07-18`. \n",
    "\n",
    "* To find all the available versions of a model, search for that model. For example: [gpt-4o-mini](https://platform.openai.com/docs/models#gpt-4o-mini)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">\n",
    "Task: Find the version of the model we used\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion[\"...\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a token?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokens can be thought of as pieces of words. These tokens are not cut up exactly where the words start or end - tokens can include trailing spaces and even sub-words. \n",
    "\n",
    "Here are some helpful rules of thumb for understanding tokens in terms of lengths:\n",
    "\n",
    "* 1 token ~= 4 chars in English\n",
    "* 1 token ~= ¾ words\n",
    "* 100 tokens ~= 75 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">\n",
    "Task: Find the number of tokens in the above example for:\n",
    "\n",
    "1. The prompt that you sent\n",
    "2. The completion that you received\n",
    "3. The total\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion[\"usage\"][\"...\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion[\"usage\"][\"...\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion[\"usage\"][\"...\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">\n",
    "Task: can you confirm these numbers using the OpenAI \n",
    "<a href=\"https://platform.openai.com/tokenizer\" style=\"color:blue\">tokenizer</a> UI?\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">\n",
    "Task: can you use\n",
    "<a href=\"https://github.com/openai/tiktoken\" style=\"color:blue\">tiktoken</a> library to count the number of prompt tokens programmatically?\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
    "tokens = encoding.encode(...)\n",
    "len(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Token Limits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the [model](https://platform.openai.com/docs/models) used, requests can use up to 128,000 tokens shared between prompt and completion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Token Pricing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requests to different models are priced differently. You can find details on token pricing [here](https://openai.com/api/pricing/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is temperature?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* You can think of temperature like randomness, with 0 being least random (most deterministic) and 2 being most random (least deterministic). \n",
    "\n",
    "* When using low values for temperature (e.g. 0.2) the model responses will tend to be more consistent but may feel more robotic. \n",
    "\n",
    "* Values higher than 1.0, especially values close to 2.0, can lead to erratic model outputs. \n",
    "\n",
    "* If your goal is creative outputs, a combination of a slightly higher than normal temperature (e.g. 1.2) combined with a prompt specifically asking the model to be creative may be your best bet, but you should experiment!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">\n",
    "Task: Change the temperature above and resubmit\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"write a dad joke and be creative\"\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt,\n",
    "        }\n",
    "    ],\n",
    "    max_tokens=100,\n",
    "    temperature=...,\n",
    ")\n",
    "\n",
    "completion = completion.to_dict()\n",
    "\n",
    "completion[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">\n",
    "Task: Create a <span style=\"color:blue\">get_completion</span> function to send a prompt to a specific model and to return the response content\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=\"gpt-4o-mini\"):\n",
    "    completion = ...\n",
    "\n",
    "    completion = completion.to_dict()\n",
    "\n",
    "    content = completion[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "    return content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://platform.openai.com/docs/models#dall-e\" style=\"color:blue\">DALL·E</a> can create realistic images and art from a prompt, edit an existing image, or create variations of a provided image.\n",
    "\n",
    "Currently, there are 2 variations: DALL·E 2 and DALL·E 3 with different options for generating images. <a href=\"https://platform.openai.com/docs/guides/images?language=python#which-model-should-i-use\" style=\"color:blue\">Which model should you use?</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.images.generate(\n",
    "    model=\"dall-e-2\",\n",
    "    prompt=\"a white siamese cat\",\n",
    "    size=\"1024x1024\",\n",
    "    n=1,\n",
    ")\n",
    "\n",
    "response = response.to_dict()\n",
    "\n",
    "response[\"data\"][0][\"url\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`model`: The model to use for image generation. Must be one of `dall-e-2` or `dall-e-3`\n",
    "\n",
    "`prompt`: description of the desired image(s). Maximum length is 1000 characters for `dall-e-2` and 4000 characters for `dall-e-3`\n",
    "\n",
    "`size`: size of the generated images. Must be one of `256x256`, `512x512`, or `1024x1024` for `dall-e-2`. Must be one of `1024x1024`, `1792x1024`, or `1024x1792` for `dall-e-3 models`.\n",
    "\n",
    "`n`: number of images to generate. Must be between 1 and 10. For `dall-e-3`, only n=1 is supported."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">\n",
    "Task: Play with the prompt above to generate different images.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Edits (DALL·E 2 only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Edit or extend an image by uploading an image and mask indicating which areas should be replaced.\n",
    "\n",
    "* This process is also known as <b>inpainting</b>.\n",
    "\n",
    "* The transparent areas of the mask indicate where the image should be edited, and the prompt should describe the full new image, <b>not just the erased area</b>.\n",
    "\n",
    "* The uploaded image and mask must both be square PNG images, less than 4MB in size, and have the same dimensions as each other. \n",
    "\n",
    "* The non-transparent areas of the mask aren't used to generate the output, so they don’t need to match the original image.\n",
    "\n",
    "* There're free tools to create a mask, for example \n",
    "<a href=\"https://onlinepngtools.com/erase-part-of-png\" style=\"color:blue\">Onlinepngtools</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.images.edit(\n",
    "    model=\"dall-e-2\",\n",
    "    image=open(\"data/sunlit_lounge.png\", \"rb\"),\n",
    "    mask=open(\"data/mask.png\", \"rb\"),\n",
    "    prompt=\"A sunlit indoor lounge area with a pool containing a flamingo\",\n",
    "    size=\"1024x1024\",\n",
    "    n=1,\n",
    ")\n",
    "\n",
    "response = response.to_dict()\n",
    "\n",
    "response[\"data\"][0][\"url\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">\n",
    "Task: Edit a different image using different masks. \n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variations (DALL·E 2 only)\n",
    "\n",
    "* Generate a variation of a given image.\n",
    "\n",
    "* The input image must be a square PNG image less than 4MB in size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.images.create_variation(\n",
    "    model=\"dall-e-2\",\n",
    "    image=open(\"data/corgi_and_cat_paw.png\", \"rb\"),\n",
    "    size=\"1024x1024\",\n",
    "    n=1,\n",
    ")\n",
    "\n",
    "response = response.to_dict()\n",
    "\n",
    "response[\"data\"][0][\"url\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">\n",
    "Task: Generate variations of a different image. \n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DALL·E 3 Prompting\n",
    "\n",
    "* The model takes in your prompt and automatically rewrites it:\n",
    "    * For safety reasons\n",
    "    * To add more detail (more detailed prompts generally result in higher quality images)\n",
    "\n",
    "* The updated prompt is visible in the `revised_prompt` field of the data response object.\n",
    "\n",
    "* You can't disable this feature, but you can get outputs closer to your requested image by adding the following to your prompt:\n",
    "\n",
    "`I NEED to test how the tool works with extremely simple prompts. DO NOT add any detail, just use it AS-IS.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.images.generate(\n",
    "    model=\"dall-e-3\",\n",
    "    prompt=\"a white siamese cat\",\n",
    "    size=\"1024x1024\",\n",
    "    n=1,\n",
    ")\n",
    "\n",
    "response = response.to_dict()\n",
    "\n",
    "response[\"data\"][0][\"url\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">\n",
    "Task: Find the `revised_prompt` in the response. \n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response[\"data\"][0][\"...\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
